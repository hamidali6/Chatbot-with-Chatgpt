{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "600df5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (22.2.2)\n",
      "Collecting pip\n",
      "  Using cached pip-23.0.1-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 22.2.2\n",
      "    Uninstalling pip-22.2.2:\n",
      "      Successfully uninstalled pip-22.2.2\n",
      "Successfully installed pip-23.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d8765ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (0.27.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->openai) (21.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.8.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb3358fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gradio\n",
      "  Downloading gradio-3.21.0-py3-none-any.whl (15.8 MB)\n",
      "     ---------------------------------------- 15.8/15.8 MB 1.5 MB/s eta 0:00:00\n",
      "Collecting mdit-py-plugins<=0.3.3\n",
      "  Downloading mdit_py_plugins-0.3.3-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.5/50.5 kB 1.3 MB/s eta 0:00:00\n",
      "Collecting markdown-it-py[linkify]>=2.0.0\n",
      "  Downloading markdown_it_py-2.2.0-py3-none-any.whl (84 kB)\n",
      "     ---------------------------------------- 84.5/84.5 kB 2.4 MB/s eta 0:00:00\n",
      "Collecting websockets>=10.0\n",
      "  Downloading websockets-10.4-cp39-cp39-win_amd64.whl (101 kB)\n",
      "     -------------------------------------- 101.4/101.4 kB 1.9 MB/s eta 0:00:00\n",
      "Collecting pydub\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Collecting huggingface-hub>=0.13.0\n",
      "  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n",
      "     -------------------------------------- 199.2/199.2 kB 2.0 MB/s eta 0:00:00\n",
      "Collecting altair>=4.2.0\n",
      "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "     -------------------------------------- 813.6/813.6 kB 2.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from gradio) (1.4.4)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from gradio) (4.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from gradio) (2.28.1)\n",
      "Collecting aiofiles\n",
      "  Downloading aiofiles-23.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting httpx\n",
      "  Downloading httpx-0.23.3-py3-none-any.whl (71 kB)\n",
      "     ---------------------------------------- 71.5/71.5 kB 1.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: fsspec in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from gradio) (2022.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from gradio) (1.21.5)\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.94.1-py3-none-any.whl (56 kB)\n",
      "     ---------------------------------------- 56.4/56.4 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pillow in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from gradio) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from gradio) (3.5.2)\n",
      "Requirement already satisfied: markupsafe in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from gradio) (2.0.1)\n",
      "Collecting ffmpy\n",
      "  Downloading ffmpy-0.3.0.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: aiohttp in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from gradio) (3.8.4)\n",
      "Collecting pydantic\n",
      "  Downloading pydantic-1.10.6-cp39-cp39-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 2.2/2.2 MB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pyyaml in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from gradio) (6.0)\n",
      "Collecting python-multipart\n",
      "  Downloading python_multipart-0.0.6-py3-none-any.whl (45 kB)\n",
      "     ---------------------------------------- 45.7/45.7 kB 2.2 MB/s eta 0:00:00\n",
      "Collecting uvicorn\n",
      "  Downloading uvicorn-0.21.0-py3-none-any.whl (57 kB)\n",
      "     ---------------------------------------- 57.8/57.8 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jinja2 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from gradio) (2.11.3)\n",
      "Collecting orjson\n",
      "  Downloading orjson-3.8.7-cp39-none-win_amd64.whl (202 kB)\n",
      "     -------------------------------------- 202.8/202.8 kB 3.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: jsonschema>=3.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from altair>=4.2.0->gradio) (4.16.0)\n",
      "Requirement already satisfied: entrypoints in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from altair>=4.2.0->gradio) (0.4)\n",
      "Requirement already satisfied: toolz in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from altair>=4.2.0->gradio) (0.11.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.0->gradio) (4.64.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.0->gradio) (3.6.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.13.0->gradio) (21.3)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Collecting linkify-it-py<3,>=1\n",
      "  Downloading linkify_it_py-2.0.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from pandas->gradio) (2022.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from pandas->gradio) (2.8.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->gradio) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->gradio) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->gradio) (21.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->gradio) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->gradio) (2.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->gradio) (1.8.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from aiohttp->gradio) (1.3.1)\n",
      "Collecting starlette<0.27.0,>=0.26.1\n",
      "  Downloading starlette-0.26.1-py3-none-any.whl (66 kB)\n",
      "     ---------------------------------------- 66.9/66.9 kB 1.2 MB/s eta 0:00:00\n",
      "Collecting httpcore<0.17.0,>=0.15.0\n",
      "  Downloading httpcore-0.16.3-py3-none-any.whl (69 kB)\n",
      "     ---------------------------------------- 69.6/69.6 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from httpx->gradio) (1.2.0)\n",
      "Collecting rfc3986[idna2008]<2,>=1.3\n",
      "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from httpx->gradio) (2022.9.14)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from matplotlib->gradio) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from matplotlib->gradio) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from matplotlib->gradio) (0.11.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from matplotlib->gradio) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from requests->gradio) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from requests->gradio) (1.26.11)\n",
      "Requirement already satisfied: click>=7.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from uvicorn->gradio) (8.0.4)\n",
      "Collecting h11>=0.8\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 1.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: colorama in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from click>=7.0->uvicorn->gradio) (0.4.5)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from httpcore<0.17.0,>=0.15.0->httpx->gradio) (3.5.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from jsonschema>=3.0->altair>=4.2.0->gradio) (0.18.0)\n",
      "Collecting uc-micro-py\n",
      "  Downloading uc_micro_py-1.0.1-py3-none-any.whl (6.2 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->gradio) (1.16.0)\n",
      "Building wheels for collected packages: ffmpy\n",
      "  Building wheel for ffmpy (setup.py): started\n",
      "  Building wheel for ffmpy (setup.py): finished with status 'done'\n",
      "  Created wheel for ffmpy: filename=ffmpy-0.3.0-py3-none-any.whl size=4693 sha256=e17d2180a3f974dfce00944c5a827a2f8dd3a73997243f00296be3ac014b944c\n",
      "  Stored in directory: c:\\users\\shahmeer\\appdata\\local\\pip\\cache\\wheels\\91\\e2\\96\\f676aa08bfd789328c6576cd0f1fde4a3d686703bb0c247697\n",
      "Successfully built ffmpy\n",
      "Installing collected packages: rfc3986, pydub, ffmpy, websockets, uc-micro-py, python-multipart, pydantic, orjson, mdurl, h11, aiofiles, uvicorn, starlette, markdown-it-py, linkify-it-py, huggingface-hub, httpcore, mdit-py-plugins, httpx, fastapi, altair, gradio\n",
      "Successfully installed aiofiles-23.1.0 altair-4.2.2 fastapi-0.94.1 ffmpy-0.3.0 gradio-3.21.0 h11-0.14.0 httpcore-0.16.3 httpx-0.23.3 huggingface-hub-0.13.2 linkify-it-py-2.0.0 markdown-it-py-2.2.0 mdit-py-plugins-0.3.3 mdurl-0.1.2 orjson-3.8.7 pydantic-1.10.6 pydub-0.25.1 python-multipart-0.0.6 rfc3986-1.5.0 starlette-0.26.1 uc-micro-py-1.0.1 uvicorn-0.21.0 websockets-10.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ad66f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02f35ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\inputs.py:27: UserWarning: Usage of gradio.inputs is deprecated, and will not be supported in the future, please import your component from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `optional` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\deprecation.py:40: UserWarning: `numeric` parameter is deprecated, and it has no effect\n",
      "  warnings.warn(value)\n",
      "C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\outputs.py:22: UserWarning: Usage of gradio.outputs is deprecated, and will not be supported in the future, please import your components from gradio.components\n",
      "  warnings.warn(\n",
      "C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\blocks.py:491: UserWarning: Theme should be a class loaded from gradio.themes\n",
      "  warnings.warn(\"Theme should be a class loaded from gradio.themes\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://c838de2478caec315e.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 393, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1059, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 868, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Shahmeer\\AppData\\Local\\Temp\\ipykernel_5460\\3807108071.py\", line 10, in chatbot\n",
      "    chat = openai.ChatCompletion.create(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 226, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 619, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 682, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://c838de2478caec315e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 393, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1059, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 868, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Shahmeer\\AppData\\Local\\Temp\\ipykernel_5460\\3807108071.py\", line 10, in chatbot\n",
      "    chat = openai.ChatCompletion.create(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 226, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 619, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 682, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 393, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1059, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 868, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Shahmeer\\AppData\\Local\\Temp\\ipykernel_5460\\3807108071.py\", line 10, in chatbot\n",
      "    chat = openai.ChatCompletion.create(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 226, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 619, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 682, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 393, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1059, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 868, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Shahmeer\\AppData\\Local\\Temp\\ipykernel_5460\\3807108071.py\", line 10, in chatbot\n",
      "    chat = openai.ChatCompletion.create(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 226, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 619, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 682, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\routes.py\", line 393, in run_predict\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 1059, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\gradio\\blocks.py\", line 868, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"C:\\Users\\Shahmeer\\AppData\\Local\\Temp\\ipykernel_5460\\3807108071.py\", line 10, in chatbot\n",
      "    chat = openai.ChatCompletion.create(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\", line 25, in create\n",
      "    return super().create(*args, **kwargs)\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\", line 153, in create\n",
      "    response, _, api_key = requestor.request(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 226, in request\n",
      "    resp, got_stream = self._interpret_response(result, stream)\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 619, in _interpret_response\n",
      "    self._interpret_response_line(\n",
      "  File \"C:\\Users\\Shahmeer\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\", line 682, in _interpret_response_line\n",
      "    raise self.handle_error_response(\n",
      "openai.error.RateLimitError: You exceeded your current quota, please check your plan and billing details.\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"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\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful and kind AI Assistant.\"},\n",
    "]\n",
    "\n",
    "def chatbot(input):\n",
    "    if input:\n",
    "        messages.append({\"role\": \"user\", \"content\": input})\n",
    "        chat = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\", messages=messages\n",
    "        )\n",
    "        reply = chat.choices[0].message.content\n",
    "        messages.append({\"role\": \"assistant\", \"content\": reply})\n",
    "        return reply\n",
    "\n",
    "inputs = gr.inputs.Textbox(lines=7, label=\"Chat with AI\")\n",
    "outputs = gr.outputs.Textbox(label=\"Reply\")\n",
    "\n",
    "gr.Interface(fn=chatbot, inputs=inputs, outputs=outputs, title=\"AI Chatbot\",\n",
    "             description=\"Ask anything you want\",\n",
    "             theme=\"compact\").launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e1376ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the best car in the world\n",
      "Conversation history token count: 26\n",
      "Conversation length: 2\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "You exceeded your current quota, please check your plan and billing details.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5460\\2608551503.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Conversation length: {conv_length}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m     response = openai.ChatCompletion.create(\n\u001b[0m\u001b[0;32m     38\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[0mmessages\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversation\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\chat_completion.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTryAgain\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    151\u001b[0m         )\n\u001b[0;32m    152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         response, _, api_key = requestor.request(\n\u001b[0m\u001b[0;32m    154\u001b[0m             \u001b[1;34m\"post\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[0murl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36mrequest\u001b[1;34m(self, method, url, params, headers, files, stream, request_id, request_timeout)\u001b[0m\n\u001b[0;32m    224\u001b[0m             \u001b[0mrequest_timeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrequest_timeout\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         )\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_interpret_response\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgot_stream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response\u001b[1;34m(self, result, stream)\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m             return (\n\u001b[1;32m--> 619\u001b[1;33m                 self._interpret_response_line(\n\u001b[0m\u001b[0;32m    620\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m                     \u001b[0mresult\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\openai\\api_requestor.py\u001b[0m in \u001b[0;36m_interpret_response_line\u001b[1;34m(self, rbody, rcode, rheaders, stream)\u001b[0m\n\u001b[0;32m    680\u001b[0m         \u001b[0mstream_error\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstream\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"error\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstream_error\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m200\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mrcode\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m             raise self.handle_error_response(\n\u001b[0m\u001b[0;32m    683\u001b[0m                 \u001b[0mrbody\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrheaders\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstream_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstream_error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m             )\n",
      "\u001b[1;31mRateLimitError\u001b[0m: You exceeded your current quota, please check your plan and billing details."
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import openai\n",
    "import os\n",
    "openai.api_key = \"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\" \n",
    "openai.organization = os.getenv(\"OPENAI_ORGANIZATION\") \n",
    "\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "max_response_tokens = 250\n",
    "token_limit= 4096\n",
    "conversation=[]\n",
    "conversation.append(system_message)\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens\n",
    "\n",
    "while(True):\n",
    "    user_input = input(\"\")     \n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "    conv_history_tokens = num_tokens_from_messages(conversation)\n",
    "    print(f\"Conversation history token count: {conv_history_tokens}\")\n",
    "    conv_length= len(conversation)\n",
    "    half = int(conv_length/2)\n",
    "    if (conv_history_tokens+max_response_tokens >= token_limit):\n",
    "        conversation = conversation[half:]\n",
    "        conversation.insert(0,system_message)\n",
    "    print(f\"Conversation length: {conv_length}\")\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages = conversation,\n",
    "        temperature=.7,\n",
    "        max_tokens=max_response_tokens,\n",
    "    )\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response['choices'][0]['message']['content']})\n",
    "    print(\"\\n\" + response['choices'][0]['message']['content'] + \"\\n\")\n",
    "    post_response=num_tokens_from_messages(conversation)\n",
    "    print(f\"Post response token count: {post_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a59a35c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tiktoken\n",
      "  Downloading tiktoken-0.3.1-cp39-cp39-win_amd64.whl (581 kB)\n",
      "     -------------------------------------- 581.4/581.4 kB 2.8 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2022.9.14)\n",
      "Installing collected packages: tiktoken\n",
      "Successfully installed tiktoken-0.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55dfcbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "openai.api_key = \"[INSERT YOUR OPENAI API KEY HERE]\"\n",
    "audio_file = open(\"sample.mp3\", \"rb\")\n",
    "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8e88904",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sample.mp3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5460\\1536775821.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mopenai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi_key\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"[INSERT YOUR OPENAI API KEY HERE]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0maudio_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"sample.mp3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"rb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mtranscript\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopenai\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranscribe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"whisper-1\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maudio_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtranscript\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample.mp3'"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"[INSERT YOUR OPENAI API KEY HERE]\"\n",
    "audio_file = open(\"sample.mp3\", \"rb\")\n",
    "transcript = openai.Audio.transcribe(\"whisper-1\", audio_file)\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da8271f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sounddevice as sd\n",
    "import wavio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "858a862d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sounddevice\n",
      "  Using cached sounddevice-0.4.6-py3-none-win_amd64.whl (199 kB)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from sounddevice) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice) (2.21)\n",
      "Installing collected packages: sounddevice\n",
      "Successfully installed sounddevice-0.4.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install sounddevice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb364793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wavio\n",
      "  Downloading wavio-0.0.7-py2.py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\shahmeer\\anaconda3\\lib\\site-packages (from wavio) (1.21.5)\n",
      "Installing collected packages: wavio\n",
      "Successfully installed wavio-0.0.7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -illow (c:\\users\\shahmeer\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install wavio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ab03fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording audio...\n",
      "Finished recording audio.\n",
      "الهلو الهلو الهلو الهلو\n"
     ]
    }
   ],
   "source": [
    "openai.api_key = \"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\"\n",
    "\n",
    "duration = 5  # adjust as needed\n",
    "fs = 44100  # sample rate\n",
    "\n",
    "# record audio from mic\n",
    "print(\"Recording audio...\")\n",
    "audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "sd.wait()  # wait until recording is finished\n",
    "print(\"Finished recording audio.\")\n",
    "\n",
    "# save recorded audio to file\n",
    "audio_file = \"audio.wav\"\n",
    "wavio.write(audio_file, audio, fs, sampwidth=2)\n",
    "\n",
    "# transcribe Arabic audio file\n",
    "with open(audio_file, \"rb\") as f:\n",
    "    transcript = openai.Audio.transcribe(\"whisper-1\", f, language=\"ar\")\n",
    "\n",
    "print(transcript.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4ff5e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import sounddevice as sd\n",
    "import wavio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e54cc04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voice_to_text():\n",
    "    openai.api_key = \"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\"\n",
    "\n",
    "    duration = 10  # adjust as needed\n",
    "    fs = 44100  # sample rate\n",
    "\n",
    "    # record audio from mic\n",
    "    print(\"Recording audio...\")\n",
    "    audio = sd.rec(int(duration * fs), samplerate=fs, channels=1)\n",
    "    sd.wait()  # wait until recording is finished\n",
    "    print(\"Finished recording audio.\")\n",
    "\n",
    "    # save recorded audio to file\n",
    "    audio_file = \"audio.wav\"\n",
    "    wavio.write(audio_file, audio, fs, sampwidth=2)\n",
    "\n",
    "    # transcribe Arabic audio file\n",
    "    with open(audio_file, \"rb\") as f:\n",
    "        transcript = openai.Audio.transcribe(\"whisper-1\", f, language=\"en\")\n",
    "\n",
    "    print(transcript.text)\n",
    "    \n",
    "    return transcript.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0db595d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording audio...\n",
      "Finished recording audio.\n",
      "Can you give me the best tourism places?\n"
     ]
    }
   ],
   "source": [
    "text = voice_to_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4ce03082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "هلو هلو هلو هلو\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4dac3876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the best wine\n",
      "Conversation history token count: 56\n",
      "Conversation length: 2\n",
      "\n",
      "I'm sorry, I cannot provide information about wine as it is not permissible in the Kingdom of Saudi Arabia. However, I can help you with information related to Hajj, tourism, work, and Umrah in KSA. Please let me know if you have any questions related to these topics.\n",
      "\n",
      "Post response token count: 121\n",
      "Can you provide me information about transportation\n",
      "Conversation history token count: 133\n",
      "Conversation length: 4\n",
      "\n",
      "Certainly! Transportation options in Saudi Arabia include taxis, buses, and metro systems. Taxis are readily available in most cities and can be hailed on the street or booked through apps like Uber or Careem. Buses are also widely available, and many long-distance routes operate between cities. In addition, Riyadh and Jeddah have metro systems that are convenient for getting around in those cities. It's important to note that women are required to sit in the back of taxis and buses, and there are separate sections for women on the metro.\n",
      "\n",
      "Post response token count: 246\n",
      "Who is the King\n",
      "Conversation history token count: 255\n",
      "Conversation length: 6\n",
      "\n",
      "As of my last update, the King of Saudi Arabia is King Salman bin Abdulaziz Al Saud. He has been the King of Saudi Arabia since January 23, 2015, and he succeeded his half-brother King Abdullah who had passed away. King Salman is the head of the Saudi Arabian royal family and the Custodian of the Two Holy Mosques in Mecca and Medina.\n",
      "\n",
      "Post response token count: 340\n",
      "Best Wine in South korea\n",
      "Conversation history token count: 351\n",
      "Conversation length: 8\n",
      "\n",
      "I'm sorry, but as an AI language model, I cannot provide information about the best wine in South Korea as I am not programmed with the latest information on wine. However, if you have any other questions related to tourism, work, Hajj, and Umrah in Saudi Arabia, I'll be happy to assist you.\n",
      "\n",
      "Post response token count: 422\n",
      "exit\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import openai\n",
    "import os\n",
    "openai.api_key = \"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\" \n",
    "openai.organization = os.getenv(\"OPENAI_ORGANIZATION\") \n",
    "\n",
    "system_message = {\"role\": \"system\", \"content\": \"You are a helpful assistant for {hajj, tourism, work and umrah} in KSA. Beside these didn't response show then a message to ask only about these topics.\"}\n",
    "max_response_tokens = 1250\n",
    "token_limit= 4096\n",
    "conversation=[]\n",
    "conversation.append(system_message)\n",
    "\n",
    "def num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n",
    "        for key, value in message.items():\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":  # if there's a name, the role is omitted\n",
    "                num_tokens += -1  # role is always required and always 1 token\n",
    "    num_tokens += 2  # every reply is primed with <im_start>assistant\n",
    "    return num_tokens\n",
    "\n",
    "while(True):\n",
    "    user_input = input(\"\")\n",
    "    \n",
    "    if user_input == \"exit\":\n",
    "        break\n",
    "    \n",
    "    if user_input == \"voice\":\n",
    "        user_text = voice_to_text()\n",
    "    else:\n",
    "        user_text = user_input\n",
    "        \n",
    "    conversation.append({\"role\": \"user\", \"content\": user_text})\n",
    "    conv_history_tokens = num_tokens_from_messages(conversation)\n",
    "    print(f\"Conversation history token count: {conv_history_tokens}\")\n",
    "    conv_length= len(conversation)\n",
    "    half = int(conv_length/2)\n",
    "    if (conv_history_tokens+max_response_tokens >= token_limit):\n",
    "        conversation = conversation[half:]\n",
    "        conversation.insert(0,system_message)\n",
    "    print(f\"Conversation length: {conv_length}\")\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages = conversation,\n",
    "        temperature=.7,\n",
    "        max_tokens=max_response_tokens,\n",
    "    )\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": response['choices'][0]['message']['content']})\n",
    "    print(\"\\n\" + response['choices'][0]['message']['content'] + \"\\n\")\n",
    "    post_response=num_tokens_from_messages(conversation)\n",
    "    print(f\"Post response token count: {post_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579db633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
